{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 190606_kerasbasics_gaussian-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rupesh05/ML_Workshop---IIT-Kanpur/blob/main/kerasbasics_gaussian_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwu_H0V1M9A6"
      },
      "source": [
        "# Keras Basics\n",
        "We will learn about\n",
        "* Dense layers\n",
        "* Categorical cross-entropy\n",
        "\n",
        "A toy example to show how to train a classifier with Keras and use it. The data comes from three gaussian distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_F5JP-dM9A_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e203893a-cf8d-4ace-ccc6-15a4a255e9ce"
      },
      "source": [
        "## DATA GENERATION\n",
        "import numpy as np\n",
        "\n",
        "def generateX(cls):\n",
        "    '''\n",
        "    Inputs:\n",
        "        cls: class {0, 1, 2}\n",
        "    Outputs:\n",
        "        a sample from cls\n",
        "    '''\n",
        "    assert cls in [0,1,2]\n",
        "    if cls==0:\n",
        "        return np.random.normal(np.array([0,0]),100)\n",
        "    elif cls==1:\n",
        "        return np.random.normal(np.array([200,200]),100)\n",
        "    elif cls==2:\n",
        "        return np.random.normal(np.array([-200,200]),100)\n",
        "    assert False\n",
        "#print(np.random.normal(np.array([0,0]),100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ -79.72652367 -108.39302036]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8C0ZHLrM9BM"
      },
      "source": [
        "Could you write a function to generate N samples from class 0 and N samples from class 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS3llbqhM9BO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "29d5120b-5ff5-476f-a29b-27867d0a5940"
      },
      "source": [
        "def generateXY(N):\n",
        "    '''\n",
        "    Inputs:\n",
        "        N: no. of samples of each class\n",
        "    '''\n",
        "    Y = []\n",
        "    X = []\n",
        "    for cls in range(3):\n",
        "        for i in range(N):\n",
        "            y = cls\n",
        "            x = generateX(y)\n",
        "            #print(x,\"kumar\")\n",
        "            x = x.reshape(-1, 1).T\n",
        "            #print(x.reshape(-1,1),\"rupesh\")\n",
        "            #print(x,\"rahul\")\n",
        "            Y.append(y)\n",
        "            X.append(x)\n",
        "            #print(X,\"rahul\")\n",
        "    X = np.concatenate(X, axis=0)\n",
        "    #print(X)\n",
        "    print(X.shape, 'X.shape')\n",
        "    print(np.array(Y).shape)\n",
        "    Y = np.array(Y).reshape(-1,1)\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "# def generateXY(N):\n",
        "#     '''\n",
        "#     Inputs:\n",
        "#         N: no. of samples of each class\n",
        "#     '''\n",
        "#     Y = [0]*N + [1]*N + [2]*N   # a list of 0s and 1s as ground truth classes  \n",
        "#     X = [generateX(y) for y in Y]  # samples corresponding to the ground truth Y\n",
        "#     X = np.vstack(X)   # arrange samples in different rows\n",
        "#     Y = np.array(Y).reshape(-1,1)\n",
        "#     return X, Y\n",
        "\n",
        "X_train, Y_train_int = generateXY(50)\n",
        "#print(X_train,\"rupesh\")\n",
        "#print(Y_train_int,\"kumar\")\n",
        "# Nx = X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 2) X.shape\n",
            "(150,)\n",
            "(150, 2)\n",
            "(150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF-wtxZlM9BX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "10c4b58f-cfb2-4b3e-a4d7-63402c46d1b0"
      },
      "source": [
        "def test_generateXY():\n",
        "    X_train, Y_train = generateXY(50)\n",
        "    assert X_train.shape==(150,2)\n",
        "    assert Y_train.shape==(150,1)\n",
        "    print(\"OK\")\n",
        "test_generateXY()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 2) X.shape\n",
            "(150,)\n",
            "(150, 2)\n",
            "(150, 1)\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtdHtV8oM9Bk"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "Now our Y is in the form [0], [1] and [2]. We want to convert them to [1,0,0], [0,1,0] and [0,0,1], respectively. \n",
        "Could you write a code to convert Y (with one column) into one-hot encoded Y (with 3 columns)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGsCK-1yM9Bl"
      },
      "source": [
        "def oneHot(y, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        y: an int in {0, 1, 2}\n",
        "        Ny: Number of classes, e.g., 3 here.\n",
        "    Output:\n",
        "        Y: a vector of Ny (=3) tuples\n",
        "    '''\n",
        "    Y = np.zeros(Ny)\n",
        "    Y[y] = 1\n",
        "    return Y\n",
        "\n",
        "Ny = 3\n",
        "Y_train = np.array([oneHot(y,Ny) for y in Y_train_int])#y_train is a string\n",
        "#print(Y_train.shape,\"kaggle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbps14rJM9Br"
      },
      "source": [
        "def test_oneHot():\n",
        "    assert np.all(oneHot(0,3)==np.array([1,0,0]))\n",
        "    assert np.all(oneHot(1,3)==np.array([0,1,0]))\n",
        "    assert np.all(oneHot(2,3)==np.array([0,0,1]))\n",
        "    assert Y_train.shape[1]==3\n",
        "    print(\"OK\")\n",
        "test_oneHot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_-e2roJM9B0"
      },
      "source": [
        "### Input Normalization\n",
        "X can lie in any unbounded range. We need to curtail to a narrow range close to zero. This helps in enhancing the stability of training and hyper-parameter tuning.\n",
        "This is normally achieved by scaling the X to have zero mean and unit standard deviation (std).\n",
        "\n",
        "$X \\leftarrow \\frac{X-mean(X)}{std(X)}$, where this is element wise division\n",
        "\n",
        "Could you use training samples to find mean and std, and normalize your X_train with that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByV7uf03M9B3"
      },
      "source": [
        "def findMeanStddev(X):\n",
        "    '''\n",
        "    Input: \n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "    Output:\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X)\n",
        "    '''\n",
        "    mean = np.sum(X, axis=0)/X.shape[0]\n",
        "    #print(X.shape[0],\"rupesh\") \n",
        "    #print(mean)   [1.53950926e-16 -8.88178420e-18] = [0 0]#here mean is 2*1 matrix\n",
        "    X1 = X-mean\n",
        "    #print(X1)\n",
        "    #print(X1.shape)\n",
        "    stddev = np.sqrt(np.sum(X1*X1, axis=0)/X.shape[0])\n",
        "    #print(stddev,\"rupesh\") stddev = [0.99999 0.9999999] = [1 1]\n",
        "    return mean, stddev\n",
        "\n",
        "def normalizeX(X, mean, stddev):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X) \n",
        "    Output:\n",
        "        Xn: X modified to have 0 mean and 1 std dev\n",
        "    Important:normalise function = [X-mean(X)/stddev(X)]\n",
        "    '''\n",
        "    Xn = (X-mean)/(stddev+1e-8)\n",
        "    #here extra term is added because there is a possiblity that stddev be 0\n",
        "    #print(Xn,\"rupesh\")\n",
        "    return Xn\n",
        "\n",
        "mean_train, stddev_train = findMeanStddev(X_train)\n",
        "X_train = normalizeX(X_train, mean_train, stddev_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyuQ0HnqM9CB"
      },
      "source": [
        "#doubt cell\n",
        "def test_normalizeX():\n",
        "    X = np.ones((3,3))\n",
        "    #print(X)\n",
        "    m,s = findMeanStddev(X)\n",
        "    #print(m,s) taken approxmately\n",
        "    assert np.all(m==np.ones(3))\n",
        "    assert np.all(s==np.zeros(3))\n",
        "    assert np.all(normalizeX(X,m,s)==0*X)\n",
        "    assert mean_train.shape==(2,)\n",
        "    assert stddev_train.shape==(2,)\n",
        "    # test on random X\n",
        "    X = np.random.random((5,3))\n",
        "    m,s = findMeanStddev(X)\n",
        "    Xn = normalizeX(X,m,s)\n",
        "    mn, sn = findMeanStddev(Xn)\n",
        "    assert np.allclose(mn, np.zeros(3))\n",
        "    assert np.allclose(sn, np.ones(3))\n",
        "    print(\"OK\")\n",
        "test_normalizeX()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idfaqf4OM9CM"
      },
      "source": [
        "### Plotting\n",
        "Could you plot all the samples in X_train with different colors for different classes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyaxtlJM9CQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "def plotXY(X, Y):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample) - normalised samples\n",
        "        Y: a matrix of size (no. of samples, no. of classes) - these are one-hot vectors\n",
        "    Action:\n",
        "        Plots the samples in X, their color depends on Y\n",
        "    '''\n",
        "    Ny = Y.shape[1]\n",
        "    for cls in range(Ny):\n",
        "        idx = np.where(Y[:,cls]==1)[0]\n",
        "        plt.plot(X[idx,0], X[idx,1], colors[cls]+'.')\n",
        "    plt.show()\n",
        "plotXY(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hb0zbGJM9Cc"
      },
      "source": [
        "## Creating the Network\n",
        "We now create the network with dense layers: \n",
        "$y = f(Wx)$\n",
        "\n",
        "ReLU activation: \n",
        "$f(h) = h, h>0; 0, h\\le 0$\n",
        "\n",
        "Softmax activation: \n",
        "$f(h_i) = \\frac{\\exp(h_i)}{\\sum_j \\exp(h_j)}$\n",
        "\n",
        "Categorical cross-entropy loss:\n",
        "$\\mathcal{L} = -\\sum_t y^d_t \\log y_t$\n",
        "\n",
        "Stochastic Gradient Descent:\n",
        "$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_bXT5x8M9Cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "1817a241-08a6-4b43-e198-adf5c23b2f8b"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "Nx = 2\n",
        "x = Input(shape=(Nx,)) \n",
        "'''\n",
        "    Input(shape=(Nx,)) - means we take input arrays of shape(*,Nx)\n",
        "    Nx is dimension and we know for every problem\n",
        "    print(x,\"rupesh\")\n",
        "    \n",
        "'''\n",
        "y = Dense(20, activation='relu')(x) \n",
        "'''\n",
        "   hidden Dense layer and \n",
        "   20 represent no of neuron in hidden layer\n",
        "   \n",
        "'''\n",
        "y = Dense(Ny, activation='softmax')(y) #output dense layer\n",
        "#model inclucde all layer between input layer and output layer\n",
        "model = Model(inputs=x, outputs=y)\n",
        "#print(model)\n",
        "#metrics: list of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy']\n",
        "model.compile(optimizer=optimizers.sgd(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 3)                 63        \n",
            "=================================================================\n",
            "Total params: 123\n",
            "Trainable params: 123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyqM6U0M9Cm"
      },
      "source": [
        "### Plotting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpiTEmxJM9Co"
      },
      "source": [
        "#studied later\n",
        "from keras.utils import plot_model\n",
        "def plotModel(model):\n",
        "    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "    from IPython.display import Image\n",
        "    Image(retina=True, filename='model.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNmbpH6tM9Cu"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3NyeI8HM9Cw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1835
        },
        "outputId": "f8ad2239-6c1a-4906-a88c-c2bf04c8682f"
      },
      "source": [
        "'''\n",
        "   X_train is normalised sample and Y_train is oneHot encoded sample\n",
        "'''\n",
        "history = model.fit(X_train, Y_train, epochs=50) # validation_split = 0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 0s 103us/step - loss: 0.9476 - acc: 0.6133\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 84us/step - loss: 0.9461 - acc: 0.6267\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 100us/step - loss: 0.9446 - acc: 0.6267\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 76us/step - loss: 0.9431 - acc: 0.6400\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 71us/step - loss: 0.9416 - acc: 0.6467\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 68us/step - loss: 0.9401 - acc: 0.6733\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 76us/step - loss: 0.9386 - acc: 0.6867\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 67us/step - loss: 0.9371 - acc: 0.7000\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 70us/step - loss: 0.9356 - acc: 0.7133\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 82us/step - loss: 0.9342 - acc: 0.7333\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 67us/step - loss: 0.9327 - acc: 0.7400\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 63us/step - loss: 0.9313 - acc: 0.7467\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 60us/step - loss: 0.9298 - acc: 0.7467\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 64us/step - loss: 0.9283 - acc: 0.7467\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 64us/step - loss: 0.9269 - acc: 0.7467\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 61us/step - loss: 0.9254 - acc: 0.7533\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 61us/step - loss: 0.9240 - acc: 0.7600\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 67us/step - loss: 0.9225 - acc: 0.7667\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 60us/step - loss: 0.9211 - acc: 0.7667\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 82us/step - loss: 0.9197 - acc: 0.7667\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 81us/step - loss: 0.9182 - acc: 0.7667\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 51us/step - loss: 0.9168 - acc: 0.7733\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 46us/step - loss: 0.9154 - acc: 0.7733\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 49us/step - loss: 0.9140 - acc: 0.7800\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 49us/step - loss: 0.9126 - acc: 0.7800\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 48us/step - loss: 0.9112 - acc: 0.7800\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 60us/step - loss: 0.9098 - acc: 0.7800\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 57us/step - loss: 0.9084 - acc: 0.7800\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 59us/step - loss: 0.9070 - acc: 0.7867\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 52us/step - loss: 0.9056 - acc: 0.7867\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 55us/step - loss: 0.9042 - acc: 0.7867\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 51us/step - loss: 0.9029 - acc: 0.7867\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 48us/step - loss: 0.9015 - acc: 0.7867\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 49us/step - loss: 0.9001 - acc: 0.7933\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 52us/step - loss: 0.8987 - acc: 0.7933\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 55us/step - loss: 0.8974 - acc: 0.7933\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 68us/step - loss: 0.8960 - acc: 0.8000\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 83us/step - loss: 0.8946 - acc: 0.8000\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 64us/step - loss: 0.8933 - acc: 0.8067\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 73us/step - loss: 0.8920 - acc: 0.8067\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 60us/step - loss: 0.8906 - acc: 0.8000\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 88us/step - loss: 0.8893 - acc: 0.8000\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 62us/step - loss: 0.8879 - acc: 0.8000\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 89us/step - loss: 0.8866 - acc: 0.8000\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 80us/step - loss: 0.8852 - acc: 0.8000\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 72us/step - loss: 0.8839 - acc: 0.8000\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 76us/step - loss: 0.8825 - acc: 0.8000\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 66us/step - loss: 0.8812 - acc: 0.8000\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 68us/step - loss: 0.8799 - acc: 0.8000\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 105us/step - loss: 0.8785 - acc: 0.8067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSWKLcsJM9C4"
      },
      "source": [
        "### Evaluation\n",
        "Could you:\n",
        "- Generate 20 samples from each class\n",
        "- Normalize them with mean_train and std_train\n",
        "- Get Y_test as one hot encoded labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3QB669hM9C7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "50d26411-19dd-4ee3-ab07-d929dd28b6d3"
      },
      "source": [
        "X_test, Y_test_int = generateXY(20)\n",
        "X_test = normalizeX(X_test, mean_train, stddev_train)\n",
        "Y_test = np.array([oneHot(y,Ny) for y in Y_test_int])\n",
        "#print(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 2) X.shape\n",
            "(60,)\n",
            "(60, 2)\n",
            "(60, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "738j_68KM9DA",
        "outputId": "4f429589-bccb-43a4-f450-5526a54f9bf7"
      },
      "source": [
        "def test_testData():\n",
        "    assert Y_test.shape==(60,3)\n",
        "    assert X_test.shape==(60,2)\n",
        "#     mn, sn = findMeanStddev(X_test)\n",
        "#     assert np.allclose(mn, np.zeros(2), atol=1)\n",
        "#     assert np.allclose(sn, np.ones(2), atol=1)\n",
        "    print(\"OK\")\n",
        "test_testData()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMtdwWHM9DI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e9e121a6-9410-401e-80f0-2f15d0f2ad3d"
      },
      "source": [
        "## Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)  # Evaluate the model\n",
        "print('Accuracy :%0.3f'%accuracy)\n",
        "Y_pred = model.predict(X_test)\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :0.783\n",
            "[[10  8  2]\n",
            " [ 0 20  0]\n",
            " [ 2  1 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}